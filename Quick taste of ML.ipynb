{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will give a brief overview of various ML topics I am experimenting with and learning about. Hope you enjoy it.\n",
    "\n",
    "I will be playing with Haberman's famous dataset concerning breast cancer surgey's eprformed at the University of chicago from 1958-1970. Data was retrievd from the UCI Machine Learning Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start with importing your libraries - yes, it's a lot, but sklearn is an absolute joy to use!\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "habermans_dataset = pd.read_csv(\"haberman.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30</th>\n",
       "      <th>64</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   30  64  1  1.1\n",
       "0  30  62  3    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "habermans_dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be better if we had descriptive column names. This can be achieved easily!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "habermans_dataset.columns = [\"patient_age\", \"operation_year\", \"nodes_detected\", \"survival_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_age</th>\n",
       "      <th>operation_year</th>\n",
       "      <th>nodes_detected</th>\n",
       "      <th>survival_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_age  operation_year  nodes_detected  survival_status\n",
       "0           30              62               3                1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "habermans_dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "habermans_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gorgeous!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we have any missing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_age 0\n",
      "operation_year 0\n",
      "nodes_detected 0\n",
      "survival_status 0\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(habermans_dataset.columns):\n",
    "    missing_data = sum(pd.isnull(habermans_dataset[val]))\n",
    "    print(habermans_dataset.columns[i], missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, no missing data! Let's proceed!\n",
    "\n",
    "Looks like we have a supervised learning problem here as we have a category - survival status. Let's check what type of classification problem this is i.e. binary or multi-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "habermans_dataset[\"survival_status\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, looks like a binary variable. 1 corresponds to a patient dying within 5 years of breast surgery and 2 cprresponds to surviving past 5 years from date of surgery\n",
    "\n",
    "Let's get to splitting our data into training and test sets! we are going to use the sklearn library - let the fun begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "array = habermans_dataset.values\n",
    "X = array[:,:3] #features!\n",
    "y = array[:,3] #outcome variable.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "test_size=0.2) #we will be using 80* of the data to train our models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a for loop that will loop through various classification algorithm's and perform a battery of tests (cross-validation) and report the accuracy! This saves us a lot of time and gives us a glimpse into perhaps which algorithm would be best suited for this dataset. Please note: this is a very clean dataset, usually we will be dealing with extremely large and disorganised datasets. It is important to take the time to understand the charactersitics of your dataset before proceeding to this step. You must ensure that you fully understand the assumptions implicit in each algorithm and it's implications for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's first create a list of the algo's we wish to test out. This will allow for quick looping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algos = []\n",
    "algos.append(('LR', LogisticRegression()))\n",
    "algos.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "algos.append(('KNN', KNeighborsClassifier()))\n",
    "algos.append(('CART', DecisionTreeClassifier()))\n",
    "algos.append(('NB', GaussianNB()))\n",
    "algos.append(('SVM', SVC()))\n",
    "algos.append(('Neural Net', MLPClassifier()))\n",
    "algos.append(('RFC', RandomForestClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.748718 (0.140509)\n",
      "LDA: 0.744551 (0.146461)\n",
      "KNN: 0.748397 (0.117009)\n",
      "CART: 0.663462 (0.141251)\n",
      "NB: 0.757051 (0.132708)\n",
      "SVM: 0.740705 (0.116703)\n",
      "Neural Net: 0.745192 (0.136719)\n",
      "RFC: 0.716346 (0.105699)\n"
     ]
    }
   ],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, algorithm in algos:\n",
    "    cross_validation = model_selection.KFold(n_splits=20)\n",
    "    eval_metrics = model_selection.cross_val_score(algorithm, X_train, y_train, cv=cross_validation, scoring='accuracy')\n",
    "    results.append(eval_metrics)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, eval_metrics.mean(), eval_metrics.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a few of our models had similar mean accuracy, however, our Naive Bayes had the lowest standard deviation. Let's proceed with that!\n",
    "\n",
    "Let's make a prediction on our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.704918032787\n",
      "[[40  2]\n",
      " [16  3]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  >=5 years       0.71      0.95      0.82        42\n",
      "   <5 years       0.60      0.16      0.25        19\n",
      "\n",
      "avg / total       0.68      0.70      0.64        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb =  GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions, target_names=[\">=5 years\",\"<5 years\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the following definitions:\n",
    "\n",
    "- Precision: true positives/(true positives + false positives). We can interpret this as Positive Predictive Value\n",
    "\n",
    "\n",
    "- Recall: true positives/(true positives + false negatives). We can interpret this as model sensitivity.\n",
    "\n",
    "\n",
    "\n",
    "- F1 Score: $2  \\times (\\frac{precision*recall}{precision + recall}) $. We can interpret this as accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
